{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass, asdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory_allocated(model, device):\n",
    "    initial_mem = torch.cuda.memory_allocated()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    final_mem = torch.cuda.memory_allocated()\n",
    "\n",
    "    mem_used = final_mem - initial_mem\n",
    "\n",
    "    print(f\"GPU Memmory used: {mem_used/ (1024 ** 2):.2f} MB\")\n",
    "\n",
    "\n",
    "def trainable_params(model):\n",
    "    total_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            total_params += param.numel()\n",
    "\n",
    "    print(f\"Trainable params: {total_params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA and BnB quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Quantize_config:\n",
    "    quant_type: str = \"nf4\"\n",
    "    compute_dtype: torch.dtype = torch.float16\n",
    "    compress_statistics: bool = False\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "@dataclass\n",
    "class LoRA_config():\n",
    "    r: int = 16\n",
    "    lora_alpha: int = 1\n",
    "    lora_dropout: float = 0.\n",
    "    merge_weights: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA_Layer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int, \n",
    "        lora_alpha: int, \n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool,\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRA_Linear(bnb.nn.Linear4bit, LoRA_Layer):\n",
    "    def __init__(self, \n",
    "                 quantize_config: Quantize_config,\n",
    "                 lora_config: LoRA_config,\n",
    "                 source_linear: nn.Linear,\n",
    "                ) -> None:\n",
    "        \n",
    "        bnb.nn.Linear4bit.__init__(self, \n",
    "                                   source_linear.in_features, \n",
    "                                   source_linear.out_features, \n",
    "                                   source_linear.bias is not None,\n",
    "                                   **asdict(quantize_config))\n",
    "        LoRA_Layer.__init__(self, **asdict(lora_config))\n",
    "        \n",
    "        if self.r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((self.r, source_linear.in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((source_linear.out_features, self.r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        self.weight = bnb.nn.Params4bit(data=source_linear.weight, \n",
    "                                        quant_type=quantize_config.quant_type, \n",
    "                                        requires_grad=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = self.forward_impl(x)\n",
    "        result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
    "        return result\n",
    "\n",
    "\n",
    "    def forward_impl(self, x: torch.Tensor):\n",
    "        if self.bias is not None and self.bias.dtype != x.dtype:\n",
    "            self.bias.data = self.bias.data.to(x.dtype)\n",
    "\n",
    "        if getattr(self.weight, \"quant_state\", None) is None:\n",
    "            if getattr(self, \"quant_state\", None) is not None:\n",
    "                assert self.weight.shape[1] == 1\n",
    "                if not isinstance(self.weight, bnb.nn.Params4bit):\n",
    "                    self.weight = bnb.nn.Params4bit(self.weight, quant_storage=self.quant_storage)\n",
    "                self.weight.quant_state = self.quant_state\n",
    "            else:\n",
    "                print(\n",
    "                    \"FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.\",\n",
    "                )\n",
    "        if not self.compute_type_is_set:\n",
    "            self.set_compute_type(x)\n",
    "            self.compute_type_is_set = True\n",
    "\n",
    "        inp_dtype = x.dtype\n",
    "        if self.compute_dtype is not None:\n",
    "            x = x.to(self.compute_dtype)\n",
    "\n",
    "        bias = None if self.bias is None else self.bias.to(self.compute_dtype)\n",
    "        out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
    "        out = out.to(inp_dtype)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peft_model(model, quantize_config, lora_config):\n",
    "    new_layers = []\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            new_layer = QLoRA_Linear(quantize_config, lora_config, layer)\n",
    "            new_layers.append(new_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)\n",
    "    return nn.Sequential(*new_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "pretrain_vgg16 = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "\n",
    "lora_classifier = get_peft_model(pretrain_vgg16.classifier, \n",
    "                                          Quantize_config(), \n",
    "                                          LoRA_config())\n",
    "\n",
    "\n",
    "class CLS_model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.features = pretrain_vgg16.features.eval()\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier =  nn.Sequential(\n",
    "            *lora_classifier,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=1000, out_features=10, bias=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLS_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memmory used: 117.69 MB\n"
     ]
    }
   ],
   "source": [
    "check_memory_allocated(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0.70 M\n"
     ]
    }
   ],
   "source": [
    "trainable_params(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 1.5365, GPU used: 0.16 G\n",
      "Epoch [2/10], Average Loss: 1.2916, GPU used: 0.16 G\n",
      "Epoch [3/10], Average Loss: 1.2154, GPU used: 0.16 G\n",
      "Epoch [4/10], Average Loss: 1.1515, GPU used: 0.16 G\n",
      "Epoch [5/10], Average Loss: 1.1128, GPU used: 0.16 G\n",
      "Epoch [6/10], Average Loss: 1.0824, GPU used: 0.16 G\n",
      "Epoch [7/10], Average Loss: 1.0618, GPU used: 0.16 G\n",
      "Epoch [8/10], Average Loss: 1.0405, GPU used: 0.16 G\n",
      "Epoch [9/10], Average Loss: 1.0250, GPU used: 0.16 G\n",
      "Epoch [10/10], Average Loss: 1.0062, GPU used: 0.16 G\n",
      "Finished Training\n",
      "Training time: 130.53 s\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(10):  \n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{10}], Average Loss: {running_loss / len(trainloader):.4f}, GPU used: {torch.cuda.memory_allocated(0)/1e9:.2f} G')\n",
    "\n",
    "print('Finished Training')\n",
    "print(f'Training time: {time.time() - start:.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 64.75 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the 10000 test images: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thuan_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
